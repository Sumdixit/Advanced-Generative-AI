{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# AI\u2011Powered HR Assistant (Nestl\u00e9 HR Policy, 2012)\nThis notebook walks through building a Retrieval\u2011Augmented Generation (RAG) chatbot:\n1) Parse PDFs\n2) Chunk & embed text\n3) Store in ChromaDB\n4) Retrieve & answer with OpenAI chat models\n5) Optional Gradio UI\n\n**Note:** Set your `OPENAI_API_KEY` in the environment before running."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "!python --version\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "%pip -q install -r /mnt/data/hr_assistant_project/requirements.txt\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os, re, hashlib\nfrom typing import List, Tuple, Dict\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport chromadb\nfrom chromadb.config import Settings\nfrom chromadb.utils import embedding_functions\nfrom pypdf import PdfReader\nfrom openai import OpenAI\n\nDEFAULT_CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")  # or \"gpt-3.5-turbo\"\nDEFAULT_EMBED_MODEL = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\nCHROMA_DIR = \"./chroma_hr_policy_db\"\n\nDOC_PATHS = [\n    \"/mnt/data/the_nestle_hr_policy_pdf_2012.pdf\",\n    \"/mnt/data/Course_End_Project_Crafting_an_AI_Powered_HR_Assistant.pdf\",\n    \"/mnt/data/Gradio_Documentation.pdf\",\n]\n\nSYSTEM_PROMPT = \"\"\"You are an HR policy assistant specialized in Nestl\u00e9's HR Policy (2012).\nAnswer ONLY from the provided documents. If not found, say you don't know. Cite filename and page.\"\"\"\n\nANSWER_PROMPT_TEMPLATE = \"\"\"Question:\n{question}\n\nRetrieved context:\n{context}\n\nAnswer from the context with citations like [source: filename.pdf p.X]. If unknown, say so.\"\"\"\n\ndef _require_api_key():\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        raise RuntimeError(\"Set OPENAI_API_KEY in your environment.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) PDF Loading & Chunking"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_pdf(path: str) -> List[Tuple[int, str]]:\n    reader = PdfReader(path)\n    pages = []\n    for i, page in enumerate(reader.pages, start=1):\n        try:\n            txt = page.extract_text() or \"\"\n        except Exception:\n            txt = \"\"\n        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n        pages.append((i, txt))\n    return pages\n\ndef chunk_text(text: str, size: int=900, overlap: int=150):\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = min(start + size, len(text))\n        chunks.append(text[start:end])\n        if end == len(text): break\n        start = max(0, end - overlap)\n    return chunks\n\nfrom dataclasses import dataclass\n@dataclass\nclass ChunkMeta:\n    source: str\n    page: int\n    chunk_id: str\n\ndef build_corpus(paths: List[str]):\n    texts, metas = [], []\n    for path in paths:\n        if not os.path.exists(path): \n            print(\"Missing:\", path); \n            continue\n        for page, txt in load_pdf(path):\n            if not txt: continue\n            for j, ch in enumerate(chunk_text(txt)):\n                cid = hashlib.sha1(f\"{path}-{page}-{j}-{len(ch)}\".encode()).hexdigest()\n                texts.append(ch)\n                metas.append(ChunkMeta(source=os.path.basename(path), page=page, chunk_id=cid))\n    return texts, metas\n\ntexts, metas = build_corpus(DOC_PATHS)\nlen(texts), metas[0]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Build / Load Vector Store (ChromaDB)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def embed_and_index(texts, metas, persist_dir=CHROMA_DIR):\n    _require_api_key()\n    client = chromadb.PersistentClient(path=persist_dir, settings=Settings(anonymized_telemetry=False))\n    try:\n        client.delete_collection(\"nestle_hr_policy\")\n    except Exception:\n        pass\n    ef = embedding_functions.OpenAIEmbeddingFunction(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model_name=DEFAULT_EMBED_MODEL\n    )\n    col = client.create_collection(\"nestle_hr_policy\", embedding_function=ef)\n    col.add(documents=texts, ids=[m.chunk_id for m in metas],\n            metadatas=[{\"source\": m.source, \"page\": m.page} for m in metas])\n    return col\n\ncol = embed_and_index(texts, metas)\ncol.count()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Retrieve + Generate"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def retrieve(query: str, k: int=5):\n    ef = embedding_functions.OpenAIEmbeddingFunction(\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n        model_name=DEFAULT_EMBED_MODEL\n    )\n    client = chromadb.PersistentClient(path=CHROMA_DIR, settings=Settings(anonymized_telemetry=False))\n    col = client.get_collection(\"nestle_hr_policy\", embedding_function=ef)\n    res = col.query(query_texts=[query], n_results=k)\n    docs = res[\"documents\"][0]\n    metas = res[\"metadatas\"][0]\n    return docs, metas\n\ndef render_context(docs, metas):\n    blocks = []\n    for d, m in zip(docs, metas):\n        blocks.append(f\"[{m['source']} p.{m['page']}] {d}\")\n    return \"\\n\\n\".join(blocks)\n\ndef answer(question: str):\n    _require_api_key()\n    client = OpenAI()\n    docs, metas = retrieve(question, k=5)\n    context = render_context(docs, metas)\n    prompt = ANSWER_PROMPT_TEMPLATE.format(question=question, context=context)\n    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt}]\n    resp = client.chat.completions.create(model=DEFAULT_CHAT_MODEL, messages=messages, temperature=0.2, max_tokens=600)\n    return resp.choices[0].message.content\n\nprint(answer(\"What are the key elements of Nestl\u00e9's Total Rewards?\"))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Optional: Launch Gradio UI"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import gradio as gr\n\ndef do_answer(q):\n    return answer(q)\n\ndemo = gr.Interface(fn=do_answer, inputs=gr.Textbox(label=\"Ask about Nestl\u00e9 HR Policy\"), outputs=\"text\", title=\"Nestl\u00e9 HR Assistant\")\n# Uncomment to launch\n# demo.launch()\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}